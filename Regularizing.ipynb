{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information-theoretic Regularization in Neural Networks\n",
    "\n",
    "### 07/11/2018, Heinke Hihn, heinke.hihn@uni-ulm.de, Ulm University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounded Rationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Maximum Expected Utility Theory an agent tries to maximize its expecte utility by following some policy $p(a|w)$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{p(a|w)} \\sum_w \\rho(w) \\sum_{a}{p(a|w)U(w, a)},\n",
    "\\end{equation}\n",
    "\n",
    "where $a$ is an action from the action space $A$ and $w$ is a world state from the world state space $W$, and $\\U(w,a)$ is a utility function. We assume that the world states are distributed according to a known and fixed distribution $\\rho(w)$ and that the world sates $w$ are finite and discrete. In the case of a single world state or world state distribution $p(w)=\\delta(w-w_0)$, the decision-making problem simplifies into a single function optimization problem $a^* = \\argmax_a \\U(a)$. In many cases, solving such optimization problems may require an exhaustive search, where simple enumeration is extremely expensive.\n",
    "\n",
    "A bounded rational decision maker (Ortega and Braun, 2013) tackles the above decision-making problem by settling on a good enough solution. Finding a bounded optimal policy requires to maximize the utility function while simultaneously remaining within some given constraints. The resulting policy is a conditional probability distribution $p(a|w)$, which essentially consists of choosing an action $a$ given a particular world state $w$. The constraints of limited information-processing resources can be formalized by setting an upper bound on the DKL (say B bits) that the decision-maker is maximally allowed to spend to transform its prior strategy into a posterior strategy through deliberation. This results in the following constrained optimization problem (Genewein et al., 2015):\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{p(a|w)} \\sum_w \\rho(w) \\sum_{a}{p(a|w)U(w, a)}, \\text{ s.t. } \\mathrm{DKL}(p(a|w)||p(a)) \\leq \\text{B}.\n",
    "\\end{equation}\n",
    "\n",
    "This constrained optimization problem can be formulated as an unconstrained problem (Ortega and Braun, 2013):\n",
    "\\begin{equation}\n",
    "\\max_{p(a|w)} \\left( \\sum_w \\rho(w) \\sum_{a}{p(a|w)U(w, a) - \\frac{1}{\\beta}\\DKL(p(a|w)||p(a))} \\right),\n",
    "\\label{eq:bounded}\n",
    "\\end{equation}\n",
    "\n",
    "where the inverse temperature $\\beta \\in \\mathbb{R}^+$ is a Lagrange multiplier that influences the trade off between expected utility gain and information cost. For $\\beta \\rightarrow \\infty$ the agent behaves perfectly rational and for $\\beta \\rightarrow 0$ the agent can only act according to the prior policy. The optimal prior policy in this case is given by the marginal $p(a) = \\sum_{w \\in W}{\\rho(w)  p(a|w)}$ (Ortega and Braun, 2013), in which case the Kullback-Leibler divergence becomes equal to the mutual information, i.e. $\\mathrm{DKL}(p(a|w)||p(a))=I(W;A)$. The solution to the above optimization problem can be found by iterating the following set of self-consistent equations (Ortega and Braun, 2013):\n",
    "\n",
    "\\begin{cases}\n",
    "\\begin{array}{rcl}\n",
    "p(a|w) &=& \\frac{1}{Z(w)}p(a) \\exp(\\beta_1 \\U(w,a)) \\\\\n",
    "p(a) &=& \\sum_w \\rho(w) p(a|w), \\\\\n",
    "\\end{array}\n",
    "\\end{cases}\n",
    "\n",
    "where $Z(w) = \\sum_a p(a) \\exp(\\beta_1 \\U(w,a)) $ is normalization factor. Computing such a normalization factor is usually computationally expensive as it involves summing over spaces with high cardinality. We avoid this by Monte Carlo approximation.\n",
    "\n",
    "### An Online Update Principle\n",
    "Let $\\theta$ be the parameters of a artifial neural network (ANN), which we wish to find through online updates. The corresponding objective function is then\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{p_\\theta(a|w)} \\left( \\sum_{w,a}\\rho(w) p_\\theta(a\\vert w)\\cdot \\underbrace{\\left[\\U(w,a) - \\frac{1}{\\beta} \\log \\frac{p_\\theta(a\\vert w)}{p_\\theta(a)} \\right]}_{=~j(w,a)} \\right),\n",
    "\\end{equation}\n",
    "where $p_\\theta(a) = \\sum_w \\rho(w)p_\\theta(a|w)$. The auxiliary function $j(w,a)$ gives the objective for a single $(w,a)$ decision path and allows us to rewrite the objective to $J(\\theta) = \\sum_{w,a}p(w,a)j(w,a)$. We can transform the derivative of the objective function into a expected value by applying the log trick and noticing that for each parametrized function $f_\\theta(x)$ it holds that $\\sum_x (\\nabla_\\theta p_\\theta(x))f_\\theta(x) = \\sum_x p_\\theta(x)(\\nabla_\\theta \\log f_\\theta(x))$. Similar to Policy Gradient methods (Sutton et al., 2000), we arrive at the following objective function at its gradient:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "J(\\theta) = \\mathbb{E}_{p(w,a)}\\left[p_\\theta(a \\vert w) j(w,a) \\right] \\\\\n",
    "\\nabla_\\theta J = \\mathbb{E}_{p(w,a)}\\left[\\nabla_\\theta \\log p_\\theta(a \\vert w) j(w,a) \\right]\n",
    "\\end{eqnarray}\n",
    "\n",
    "which we can approximate with Monte Carlo batches due to the expectation operator.\n",
    "\n",
    "##### Approximating the Prior Policy\n",
    "The prior policy is given by the marginal of the posterior policies, i.e. $p(a) = \\sum_w p(w) p(a|w)$. Its' computation requires summing over all states, which can be infeasible for large state spaces. Instead, we keep a exopential running average $\\hat{p}(a)$ of the posterior policies $p(a|w)$ (Ortega et al. 2015):\n",
    "\n",
    "$$\\hat{p}_{t+1} (a)= \\lambda\\hat{p}_{t}(a) + (1.0 - \\lambda)p(a|w),$$\n",
    "\n",
    "where $\\lambda$ is a decay parameter we set close to 1, e.g. 0.99. Note, that this is feasible only in the discrete action case, such as in classification. For continous actions, we keep a running average over the parameters that form the action distribution, e.g. the mean and variance of a gaussian distribution. Using this approximation, the auxillary function $j(w,a)$ becomes $\\U(w,a) - \\frac{1}{\\beta} \\log \\frac{p_\\theta(a\\vert w)}{\\hat{p}(a)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Regularization: Weight Decay\n",
    "Regularization techniques are designed to reduce the overfitting of a classifier (or regressor) during training. Overfitting means, simply put, to learn the input-output relation perfectly. The most common one is weight decay, whereby a penalty is added to the utility function. The penalty is proportional to the L2 norm of the weights. Let $\\U(w,a)$ be the utility function we want to optimize, e.g. classification error. The regularized objective function is then\n",
    "\n",
    "\\begin{eqnarray} \\U_{\\mathrm{reg}}(w,a) = -\\U(w,a) + \\frac{\\lambda}{2n} \\sum_\\Theta \\theta^2,\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\lambda$ is the regularization factor, and $\\Theta$ is the set of weight of the model. In the case of categorical cross entropy, this becomes\n",
    "\n",
    "\\begin{eqnarray} \\U_{\\mathrm{reg}}(w,a) = -\\U(w,a)\\log(p(a|w)) + \\frac{\\lambda}{2n} \\sum_\\Theta \\theta^2.\n",
    "\\tag{85}\\end{eqnarray}\n",
    "\n",
    "This formulazation forces the model to have weights close to zero, hence the term weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Imlementation is done in Keras (Chollet et al., 2013). To keep the notation consistent, we will refer to inputs as (world states) $w$ and predictions as (actions) $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Average Layers\n",
    "Running Average Layers are implemented as a sub-class of Dense Layers with a variable for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "class ExponentialMovingAverage(Layer):\n",
    "    \"\"\"Keeps an exponential average of the input variables.\n",
    "\n",
    "    This is useful to mitigate overfitting\n",
    "    (you could see it as a Bounded Rational Decision Maker where the running averages are the\n",
    "    prior strategies.)\n",
    "\n",
    "    As it is a regularization layer, it is only active at training time.\n",
    "\n",
    "    # Arguments\n",
    "        momentum: float, momentum of the epxonential decay\n",
    "        initializers: string, how to initialize the variables\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units=1, momentum=0.99, initiliazier='ones', **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ExponentialMovingAverage, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.momentum = momentum\n",
    "        self.units = units\n",
    "        self.axis = -1\n",
    "        self.initializer = initiliazier\n",
    "        self.values = None\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[self.axis]\n",
    "\n",
    "        self.values = self.add_weight(shape=(1, self.units),\n",
    "                                      initializer=self.initializer,\n",
    "                                      name='erm',\n",
    "                                      trainable=False)\n",
    "\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        reduction_axes = list(range(len(input_shape)))\n",
    "        del reduction_axes[self.axis]\n",
    "\n",
    "        def update_erm():\n",
    "            normed_training, mean, variance = K.normalize_batch_in_training(x=inputs, beta=None, gamma=None,\n",
    "                                                                            reduction_axes=reduction_axes)\n",
    "            self.add_update([K.moving_average_update(self.values,\n",
    "                                                     mean,\n",
    "                                                     self.momentum)],\n",
    "                            inputs=inputs)\n",
    "            return self.values\n",
    "\n",
    "        return K.in_train_phase(update_erm(), self.values, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'momentum': self.momentum}\n",
    "        base_config = super(ExponentialMovingAverage, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classifier\n",
    "The classifier is a feed-forward neural network following the current state fo the art architectures. As we are dealing images, we use 2D convolutions with relu nonlinearities and tanh nonlinearities for the dense layers. The corresponding output class is found a softmax axtivation function. The weight are initialized according to the Xavier scheme (He et al., 2015). We use ADAM (Kingma and Ba, 2014) to optimize the parameters of the network. The loss function is a categorical cross-entropy with the DKL as a penalty:\n",
    "\n",
    "$$-\\sum_{w=1}^M\\U(w,a)\\log(p(a|w)) + \\log(p(a|w)) \\frac{\\log(p(a|w))}{\\hat{p}(a)},$$\n",
    "\n",
    "where $\\U(w,a)$ is binary indication function, that is 1 if the predicted class $a$ is correct for input $w$ and 0 else, and $M$ is the number of classes. In this example we look at the MNIST dataset (LeCun 1998), so $M = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.optimizers import *\n",
    "from ExponentialMovingAverageLayer import *\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def create_image_classifier(input_shape, num_classes, beta):\n",
    "    def cat_cross_beta(beta):\n",
    "        beta_var = K.variable(beta)\n",
    "\n",
    "        def categorical_crossentropy(target, output, from_logits=False, b=beta_var):\n",
    "            \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n",
    "\n",
    "            # Arguments\n",
    "                target: A tensor of the same shape as `output`.\n",
    "                output: A tensor resulting from a softmax\n",
    "                    (unless `from_logits` is True, in which\n",
    "                    case `output` is expected to be the logits).\n",
    "                from_logits: Boolean, whether `output` is the\n",
    "                    result of a softmax, or is a tensor of logits.\n",
    "\n",
    "            # Returns\n",
    "                Output tensor.\n",
    "            \"\"\"\n",
    "            # Note: tf.nn.softmax_cross_entropy_with_logits\n",
    "            # expects logits, Keras expects probabilities.\n",
    "            clipped_y = K.clip(output, K.epsilon(), 1)\n",
    "            kl_loss = clipped_y * (K.log(clipped_y / K.clip(erm, K.epsilon(), 1)))\n",
    "            kl_loss = (K.variable(1.0) / K.variable(beta)) * K.mean(kl_loss)\n",
    "\n",
    "            if not from_logits:\n",
    "                # scale preds so that the class probas of each sample sum to 1\n",
    "                output /= tf.reduce_sum(output,\n",
    "                                        len(output.get_shape()) - 1,\n",
    "                                        True)\n",
    "                # manual computation of crossentropy\n",
    "                output = tf.clip_by_value(output, K.epsilon(), 1. - K.epsilon())\n",
    "                return - tf.reduce_sum(target * tf.log(output),\n",
    "                                       len(output.get_shape()) - 1) - kl_loss\n",
    "            else:\n",
    "                return tf.nn.softmax_cross_entropy_with_logits(labels=target,\n",
    "                                                               logits=output) - kl_loss\n",
    "\n",
    "        return categorical_crossentropy\n",
    "\n",
    "    inlayer = Input(shape=input_shape)\n",
    "    h = Conv2D(32, kernel_size=(3, 3),\n",
    "               activation='relu', kernel_initializer='he_normal', use_bias=True)(inlayer)\n",
    "    h = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', use_bias=True)(h)\n",
    "    h = Flatten()(h)\n",
    "    h = Dense(128, activation='tanh', kernel_initializer='he_normal', use_bias=True)(h)\n",
    "    h = Dense(num_classes, activation='softmax', kernel_initializer='he_normal', use_bias=True)(h)\n",
    "    erm = ExponentialMovingAverage(units=num_classes, name=\"conv_erm\")(h)\n",
    "    model = Model(inlayer, [h, erm])\n",
    "\n",
    "    model.compile(loss=[cat_cross_beta(beta=beta), None],\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "Running this code may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from BoundedImageClassifier import *\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('text', usetex=True)\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "beta = 125\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "idx = np.random.choice(len(x_train), 5000)\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]\n",
    "idx = np.random.choice(len(x_train), 1000)\n",
    "y_test = y_test[idx]\n",
    "x_test = x_test[idx]\n",
    "\n",
    "model = create_image_classifier(input_shape=input_shape, beta=beta, num_classes=num_classes)\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 verbose=2,\n",
    "                 validation_data=(x_test, y_test))\n",
    "\n",
    "plt.plot(hist.history['dense_2_acc'], label='Training Accuracy')\n",
    "plt.plot(hist.history['val_dense_2_acc'], label='Validation Accuracy')\n",
    "plt.title(r\"Training and Validation Loss for \\beta = %.3f\" % beta)\n",
    "plt.legend()\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score)\n",
    "print('Test accuracy:', score)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genewein, T., Leibfried, F., Grau-Moya, J., & Braun, D. A. (2015). Bounded rationality, abstraction, and hierarchical decision-making: An information-theoretic optimality principle. Frontiers in Robotics and AI, 2, 27.\n",
    "\n",
    "\n",
    "Ortega, P. A., Braun, D. A., Dyer, J., Kim, K. E., & Tishby, N. (2015). Information-Theoretic Bounded Rationality. arXiv preprint arXiv:1512.06789.\n",
    "\n",
    "Sutton, R. S., McAllester, D. A., Singh, S. P., & Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems (pp. 1057-1063).\n",
    "\n",
    "Ortega, P. A., & Braun, D. A. (2013). Thermodynamics as a theory of decision-making with information-processing costs. Proc. R. Soc. A, 469(2153), 20120683.\n",
    "\n",
    "Chollet, F. (2015). Keras. https://keras.io\n",
    "\n",
    "Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n",
    "\n",
    "LeCun, Y. (1998). The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision (pp. 1026-1034)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
